# -*- coding: utf-8 -*-
"""Untitled18.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QqQ8qLlNFHHr-DXlOSGghjCmXnRzm6Rh
"""

from google.colab import files

uploaded = files.upload()

import zipfile

zip_path = 'fake-dataset.zip'  # name of uploaded file
extract_folder = 'fake_news_data'

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_folder)

print("Files extracted!")

import pandas as pd

fake_df = pd.read_csv(f'{extract_folder}/Fake.csv')
real_df = pd.read_csv(f'{extract_folder}/True.csv')

fake_df['label'] = 1
real_df['label'] = 0

df = pd.concat([fake_df, real_df], ignore_index=True).sample(frac=1, random_state=42).reset_index(drop=True)

print(df.head())
print(df['label'].value_counts())

import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score
from nltk.stem import WordNetLemmatizer

# Download stopwords once
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

# Load your combined dataset df here, e.g. after merging Fake.csv and True.csv
# Assuming df with columns: 'text' (news article), 'label' (0=real, 1=fake)

# 1. Text Preprocessing
def preprocess(text):
    text = re.sub(r'[^a-zA-Z]', ' ', text)  # Remove non-alpha
    text = text.lower()                     # Lowercase
    words = text.split()
    stop_words = set(stopwords.words('english'))
    words = [w for w in words if w not in stop_words]  # Remove stopwords
    ps = PorterStemmer()
    words = [ps.stem(w) for w in words]   # Stemming
    return ' '.join(words)


lemmatizer = WordNetLemmatizer()

def preprocess_lemmatize(text):
    text = re.sub(r'[^a-zA-Z]', ' ', text).lower()
    words = text.split()
    stop_words = set(stopwords.words('english'))
    words = [w for w in words if w not in stop_words]
    words = [lemmatizer.lemmatize(w) for w in words]
    return ' '.join(words)


df['clean_text'] = df['text'].apply(preprocess_lemmatize)

# 2. Feature Extraction using TF-IDF
tfidf = TfidfVectorizer(max_features=5000,ngram_range=(1,2))
X = tfidf.fit_transform(df['clean_text'])

# 3. Train/Test Split
y = df['label']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 4. Binary Classification Model (Logistic Regression)
model = LogisticRegression(class_weight='balanced', max_iter=1000)
model.fit(X_train, y_train)

# 5. Prediction and Evaluation
y_pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred, target_names=['Real', 'Fake']))

import matplotlib.pyplot as plt

# Class distribution
df['label'].value_counts().plot(kind='bar', title='Class Distribution (Real=0, Fake=1)')
plt.show()

from sklearn.metrics import ConfusionMatrixDisplay

ConfusionMatrixDisplay.from_estimator(model, X_test_tfidf, y_test)

import numpy as np

feature_names = tfidf.get_feature_names_out()
coefs = model.coef_[0]

# Top features indicating "fake"
top_fake = np.argsort(coefs)[-10:]
print("Top words indicating FAKE news:")
for i in reversed(top_fake):
    print(f"{feature_names[i]}: {coefs[i]:.4f}")

# Top features indicating "real"
top_real = np.argsort(coefs)[:10]
print("\nTop words indicating REAL news:")
for i in top_real:
    print(f"{feature_names[i]}: {coefs[i]:.4f}")